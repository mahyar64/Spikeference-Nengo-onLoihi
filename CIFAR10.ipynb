{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfkPDvLkvXfYMbR45Y55JS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahyar64/Spikeference-Nengo-onLoihi/blob/main/CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nengo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Pf8FxSNRlD",
        "outputId": "429fbef6-35b9-4a7a-da44-4e247e080043"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nengo\n",
            "  Downloading nengo-3.2.0-py3-none-any.whl (535 kB)\n",
            "\u001b[K     |████████████████████████████████| 535 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from nengo) (1.21.6)\n",
            "Installing collected packages: nengo\n",
            "Successfully installed nengo-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nengo-dl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAAQGUYZoiu8",
        "outputId": "a08e957e-3f15-44bd-e38a-4db09db36650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nengo-dl\n",
            "  Downloading nengo-dl-3.5.0.tar.gz (289 kB)\n",
            "\u001b[K     |████████████████████████████████| 289 kB 7.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf<4.21.0rc0 in /usr/local/lib/python3.7/dist-packages (from nengo-dl) (3.17.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from nengo-dl) (21.3)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from nengo-dl) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: nengo>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from nengo-dl) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from nengo-dl) (1.21.6)\n",
            "Collecting progressbar2>=3.39.0\n",
            "  Downloading progressbar2-4.0.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from nengo-dl) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->nengo-dl) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->nengo-dl) (3.0.9)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2>=3.39.0->nengo-dl) (3.3.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4.21.0rc0->nengo-dl) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (1.48.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (0.26.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (0.5.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (2.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (14.0.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->nengo-dl) (1.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->nengo-dl) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0->nengo-dl) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (3.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->nengo-dl) (3.2.0)\n",
            "Building wheels for collected packages: nengo-dl\n",
            "  Building wheel for nengo-dl (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nengo-dl: filename=nengo_dl-3.5.0-py3-none-any.whl size=206240 sha256=f531a4001e25dc1f6d919c1dc82500866964b2f8deb619049ac5087545ae0b8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/a1/ab/4788317a80cbd5a999e6f70037ceadf2ce4c57e0bcf5842c2a\n",
            "Successfully built nengo-dl\n",
            "Installing collected packages: progressbar2, nengo-dl\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "Successfully installed nengo-dl-3.5.0 progressbar2-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qiXPb1hKBW6",
        "outputId": "f48b9e29-ae1c-4ba0-aaca-aa635ae8928e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.48.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.7)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nengo-extras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4XN3b5vDQOD",
        "outputId": "7073c7ea-1d92-4c70-a1f6-22819cffbaf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nengo-extras\n",
            "  Downloading nengo_extras-0.5.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from nengo-extras) (7.1.2)\n",
            "Requirement already satisfied: matplotlib>=1.4 in /usr/local/lib/python3.7/dist-packages (from nengo-extras) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from nengo-extras) (1.21.6)\n",
            "Requirement already satisfied: nengo>=3.0 in /usr/local/lib/python3.7/dist-packages (from nengo-extras) (3.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4->nengo-extras) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4->nengo-extras) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4->nengo-extras) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4->nengo-extras) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->nengo-extras) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.4->nengo-extras) (1.15.0)\n",
            "Installing collected packages: nengo-extras\n",
            "Successfully installed nengo-extras-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nengo-loihi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AZKQa_ja4-B",
        "outputId": "53e01d7a-0335-47e2-8965-c06f2784f523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nengo-loihi\n",
            "  Downloading nengo_loihi-1.1.0-py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from nengo-loihi) (2.11.3)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from nengo-loihi) (1.7.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from nengo-loihi) (21.3)\n",
            "Requirement already satisfied: nengo>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from nengo-loihi) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from nengo>=3.1.0->nengo-loihi) (1.21.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->nengo-loihi) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->nengo-loihi) (3.0.9)\n",
            "Installing collected packages: nengo-loihi\n",
            "Successfully installed nengo-loihi-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMfwAJneoKYJ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import collections\n",
        "import functools\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nengo\n",
        "import nengo_dl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import nengo_loihi\n",
        "\n",
        "\n",
        "def partial(func, *args, **kwargs):\n",
        "    \"\"\"Helper to call functools.partial and copy over func.__name__\"\"\"\n",
        "    new_func = functools.partial(func, *args, **kwargs)\n",
        "    functools.update_wrapper(new_func, func)\n",
        "    return new_func"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def percentile_l2_loss_range(\n",
        "    y_true, y, sample_weight=None, min_rate=0.0, max_rate=np.inf, percentile=99.0\n",
        "):\n",
        "    # y axes are (batch examples, time (==1), neurons)\n",
        "    assert len(y.shape) == 3\n",
        "    rates = tfp.stats.percentile(y, percentile, axis=(0, 1))\n",
        "    low_error = tf.maximum(0.0, min_rate - rates)\n",
        "    high_error = tf.maximum(0.0, rates - max_rate)\n",
        "    loss = tf.nn.l2_loss(low_error + high_error)\n",
        "\n",
        "    return (sample_weight * loss) if sample_weight is not None else loss\n",
        "\n",
        "\n",
        "def slice_data_dict(data, slice_):\n",
        "    return {key: value[slice_] for key, value in data.items()}"
      ],
      "metadata": {
        "id": "54m5V6nYwc4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define a custom class for iterating an image dataset and returning dictionaries that can be used by NengoDL for training. Much of this is a re-implementation of tf.keras.preprocessing.image.NumpyArrayIterator, with the additional features of (a) allowing us to return dictionaries with the provided keys, rather than just lists of Numpy arrays, and (b) allowing multiple y values to be returned."
      ],
      "metadata": {
        "id": "4y26eULfwtrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NengoImageIterator(tf.keras.preprocessing.image.Iterator):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_data_generator,\n",
        "        x_keys,\n",
        "        x,\n",
        "        y_keys,\n",
        "        y,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        sample_weight=None,\n",
        "        seed=None,\n",
        "        subset=None,\n",
        "        dtype=\"float32\",\n",
        "    ):\n",
        "        assert subset is None, \"Not Implemented\"\n",
        "        assert isinstance(x_keys, (tuple, list))\n",
        "        assert isinstance(y_keys, (tuple, list))\n",
        "        assert isinstance(x, (tuple, list))\n",
        "        assert isinstance(y, (tuple, list))\n",
        "\n",
        "        self.dtype = dtype\n",
        "        self.x_keys = x_keys\n",
        "        self.y_keys = y_keys\n",
        "\n",
        "        x0 = x[0]\n",
        "        assert all(len(xx) == len(x0) for xx in x), (\n",
        "            \"All of the arrays in `x` should have the same length. \"\n",
        "            \"[len(xx) for xx in x] = %s\" % ([len(xx) for xx in x],)\n",
        "        )\n",
        "        assert all(len(yy) == len(x0) for yy in y), (\n",
        "            \"All of the arrays in `y` should have the same length as `x`. \"\n",
        "            \"len(x[0]) = %d, [len(yy) for yy in y] = %s\"\n",
        "            % (len(x0), [len(yy) for yy in y])\n",
        "        )\n",
        "        assert len(x_keys) == len(x)\n",
        "        assert len(y_keys) == len(y)\n",
        "\n",
        "        if sample_weight is not None and len(x0) != len(sample_weight):\n",
        "            raise ValueError(\n",
        "                \"`x[0]` (images tensor) and `sample_weight` \"\n",
        "                \"should have the same length. \"\n",
        "                \"Found: x.shape = %s, sample_weight.shape = %s\"\n",
        "                % (np.asarray(x0).shape, np.asarray(sample_weight).shape)\n",
        "            )\n",
        "\n",
        "        self.x = [\n",
        "            np.asarray(xx, dtype=self.dtype if i == 0 else None)\n",
        "            for i, xx in enumerate(x)\n",
        "        ]\n",
        "        if self.x[0].ndim != 4:\n",
        "            raise ValueError(\n",
        "                \"Input data in `NumpyArrayIterator` \"\n",
        "                \"should have rank 4. You passed an array \"\n",
        "                \"with shape\",\n",
        "                self.x[0].shape,\n",
        "            )\n",
        "\n",
        "        self.y = [np.asarray(yy) for yy in y]\n",
        "        self.sample_weight = (\n",
        "            None if sample_weight is None else np.asarray(sample_weight)\n",
        "        )\n",
        "        self.image_data_generator = image_data_generator\n",
        "        super().__init__(self.x[0].shape[0], batch_size, shuffle, seed)\n",
        "\n",
        "    def _get_batches_of_transformed_samples(self, index_array):\n",
        "        images = self.x[0]\n",
        "        assert images.dtype == self.dtype\n",
        "\n",
        "        n = len(index_array)\n",
        "        batch_x = np.zeros((n,) + images[0].shape, dtype=self.dtype)\n",
        "        for i, j in enumerate(index_array):\n",
        "            x = images[j]\n",
        "            params = self.image_data_generator.get_random_transform(x.shape)\n",
        "            x = self.image_data_generator.apply_transform(x, params)\n",
        "            x = self.image_data_generator.standardize(x)\n",
        "            batch_x[i] = x\n",
        "\n",
        "        batch_x_miscs = [xx[index_array] for xx in self.x[1:]]\n",
        "        batch_y_miscs = [yy[index_array] for yy in self.y]\n",
        "\n",
        "        x_pairs = [\n",
        "            (k, self.x_postprocess(k, v))\n",
        "            for k, v in zip(self.x_keys, [batch_x] + batch_x_miscs)\n",
        "        ]\n",
        "        y_pairs = [\n",
        "            (k, self.y_postprocess(k, v)) for k, v in zip(self.y_keys, batch_y_miscs)\n",
        "        ]\n",
        "\n",
        "        output = (\n",
        "            collections.OrderedDict(x_pairs),\n",
        "            collections.OrderedDict(y_pairs),\n",
        "        )\n",
        "\n",
        "        if self.sample_weight is not None:\n",
        "            output += (self.sample_weight[index_array],)\n",
        "        return output\n",
        "\n",
        "    def x_postprocess(self, key, x):\n",
        "        return x if key == \"n_steps\" else x.reshape((x.shape[0], 1, -1))\n",
        "\n",
        "    def y_postprocess(self, key, y):\n",
        "        return y.reshape((y.shape[0], 1, -1))"
      ],
      "metadata": {
        "id": "UW1n0Rilwvwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the cifar-10 dataset\n",
        " range [0, 255], rescaled to the range [-1, 1] and images are flattened into vectors"
      ],
      "metadata": {
        "id": "KFW6_KSJxGD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channels_last = True\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "n_classes = len(np.unique(train_y))\n",
        "\n",
        "# TensorFlow does not include the label names, so define them manually\n",
        "label_names = [\n",
        "    \"airplane\",\n",
        "    \"automobile\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        "]\n",
        "assert n_classes == len(label_names)\n",
        "\n",
        "if not channels_last:\n",
        "    train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
        "    test_x = np.transpose(test_x, (0, 3, 1, 2))\n",
        "\n",
        "# convert the images to float32, and rescale to [-1, 1]\n",
        "train_x = train_x.astype(np.float32) / 127.5 - 1\n",
        "test_x = test_x.astype(np.float32) / 127.5 - 1\n",
        "\n",
        "train_t = np.array(\n",
        "    tf.one_hot(train_y, n_classes, on_value=1, off_value=0), dtype=np.float32\n",
        ")\n",
        "test_t = np.array(\n",
        "    tf.one_hot(test_y, n_classes, on_value=1, off_value=0), dtype=np.float32\n",
        ")\n",
        "\n",
        "train_y = train_y.squeeze()\n",
        "test_y = test_y.squeeze()\n",
        "\n",
        "train_x_flat = train_x.reshape((train_x.shape[0], 1, -1))\n",
        "train_t_flat = train_t.reshape((train_t.shape[0], 1, -1))\n",
        "\n",
        "test_x_flat = test_x.reshape((test_x.shape[0], 1, -1))\n",
        "test_t_flat = test_t.reshape((test_t.shape[0], 1, -1))\n",
        "\n",
        "input_shape = nengo.transforms.ChannelShape(\n",
        "    test_x[0].shape, channels_last=channels_last\n",
        ")\n",
        "assert input_shape.n_channels in (1, 3)\n",
        "assert train_x[0].shape == test_x[0].shape == input_shape.shape"
      ],
      "metadata": {
        "id": "2P5UlvzRxThg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c647808f-a620-48f3-ef86-ac80bcec1bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Nengo Network"
      ],
      "metadata": {
        "id": "4N7Ugz4zyu8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_rate = 150                            # max_rate is the target maximum firing rate for all ensembles, above 150 Hz, the quantization error in Loihi neurons becomes significant\n",
        "amp = 1.0 / max_rate                      # so that neuron outputs are generally between 0 and 1; \n",
        "rate_reg = 1e-3                          # must be in amplitude scaled units, rate_reg is the amount of regularization on the firing rates, \n",
        "rate_target = max_rate * amp              \n",
        "                                          #The rate_target is the target value used in the loss functions\n",
        "\n",
        "relu = nengo.SpikingRectifiedLinear(amplitude=amp)            # input neurons that are run off-chip\n",
        "chip_neuron = nengo_loihi.neurons.LoihiLIF(amplitude=amp)     # on-chip neurons takes into account the quantization error in Loihi neurons\n",
        "\n",
        "\n",
        "#create the network, #Convolutional and dense neural layers that are run on the chip (Loihi), \n",
        "#split the image spatially, the shape of the image being represented is (15, 15, 64) (that is, 15 rows and columns, and 64 channels)\n",
        "\n",
        "layer_confs = [                                                \n",
        "    dict(\n",
        "        name=\"input-layer\",     #Input neurons that are run off-chip to turn the input into spikes\n",
        "        filters=4,             #transform to turn those 3 color values into 4 values, and then pass that into ReLu to generate spikes\n",
        "        kernel_size=1,         \n",
        "        strides=1,\n",
        "        neuron=relu,\n",
        "        on_chip=False,\n",
        "    ),\n",
        "    dict(name=\"conv-layer1\", filters=64, kernel_size=3, strides=2, block=(8, 8, 16)),    #the block shape (8, 8, 8), which means that blocks will have to be tiled twice both row-wise\n",
        "                                                                                         #and column-wise to represent the spatial extent of the image.  \n",
        "    dict(name=\"conv-layer2\", filters=72, kernel_size=3, strides=1, block=(7, 7, 8)),      \n",
        "    dict(name=\"conv-layer3\", filters=256, kernel_size=3, strides=2, block=(6, 6, 12)),    #Block is the size of representation that will go on one “block” (i.e. Loihi core), \n",
        "                                                                                          #in (rows, columns, channels) format\n",
        "    dict(name=\"conv-layer4\", filters=256, kernel_size=1, strides=1, block=(6, 6, 24)),    #Loihi cores are limited to 1024 neurons\n",
        "    dict(name=\"conv-layer5\", filters=64, kernel_size=1, strides=1, block=(6, 6, 24)),\n",
        "    dict(name=\"dense-layer\", n_neurons=100, block=(50,)),         \n",
        "    dict(name=\"output-layer\", n_neurons=10, neuron=None, on_chip=False),\n",
        "]\n",
        "\n",
        "# Create a PresentInput process to show images from the training set sequentially.\n",
        "# Each image is presented for `presentation_time` seconds.\n",
        "# NOTE: this is not used during training, since we get `nengo_dl` to override the\n",
        "# output of this node with the training data.\n",
        "presentation_time = 0.2\n",
        "present_images = nengo.processes.PresentInput(test_x_flat, presentation_time)\n",
        "\n",
        "total_n_neurons = 0\n",
        "total_n_weights = 0\n",
        "\n",
        "with nengo.Network() as net:\n",
        "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([max_rate])\n",
        "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
        "    net.config[nengo.Connection].synapse = None\n",
        "\n",
        "    # we set the learning phase to True (training) to always use rate neurons\n",
        "    nengo_dl.configure_settings(learning_phase=True)\n",
        "\n",
        "    # add a configurable keep_history option to Probes (we'll set this\n",
        "    # to False for some probes below)\n",
        "    nengo_dl.configure_settings(keep_history=True)\n",
        "\n",
        "    # this is an optimization to improve the training speed,\n",
        "    # since we won't require stateful behaviour in this example\n",
        "    nengo_dl.configure_settings(stateful=False)\n",
        "\n",
        "    # this sets the amount of smoothing used on the LIF neurons during training\n",
        "    nengo_dl.configure_settings(lif_smoothing=0.01)\n",
        "\n",
        "    # this allows us to set `nengo_loihi` parameters like `on_chip` and `block_shape`\n",
        "    nengo_loihi.add_params(net)\n",
        "\n",
        "    # the input node that will be used to feed in input images\n",
        "    inp = nengo.Node(present_images, label=\"input_node\")\n",
        "\n",
        "    connections = []\n",
        "    transforms = []\n",
        "    layer_probes = []\n",
        "    shape_in = input_shape\n",
        "    x = inp\n",
        "    for k, layer_conf in enumerate(layer_confs):\n",
        "        layer_conf = dict(layer_conf)  # copy, so we don't modify the original\n",
        "        name = layer_conf.pop(\"name\")\n",
        "        neuron_type = layer_conf.pop(\"neuron\", chip_neuron)\n",
        "        on_chip = layer_conf.pop(\"on_chip\", True)\n",
        "        block = layer_conf.pop(\"block\", None)\n",
        "\n",
        "        if block is not None and not channels_last:\n",
        "            # move channels to first index\n",
        "            block = (block[-1],) + block[:-1]\n",
        "\n",
        "        # --- create layer transform\n",
        "        if \"filters\" in layer_conf:\n",
        "            # convolutional layer\n",
        "            n_filters = layer_conf.pop(\"filters\")\n",
        "            kernel_size = layer_conf.pop(\"kernel_size\")\n",
        "            strides = layer_conf.pop(\"strides\", 1)\n",
        "            assert len(layer_conf) == 0, \"Unused fields in conv layer: %s\" % list(\n",
        "                layer_conf\n",
        "            )\n",
        "\n",
        "            kernel_size = (\n",
        "                (kernel_size, kernel_size)\n",
        "                if isinstance(kernel_size, int)\n",
        "                else kernel_size\n",
        "            )\n",
        "            strides = (strides, strides) if isinstance(strides, int) else strides\n",
        "\n",
        "            transform = nengo.Convolution(\n",
        "                n_filters=n_filters,\n",
        "                input_shape=shape_in,\n",
        "                kernel_size=kernel_size,\n",
        "                strides=strides,\n",
        "                padding=\"valid\",\n",
        "                channels_last=channels_last,\n",
        "                init=nengo_dl.dists.Glorot(scale=1.0 / np.prod(kernel_size)),\n",
        "            )\n",
        "            shape_out = transform.output_shape\n",
        "\n",
        "            loc = \"chip\" if on_chip else \"host\"\n",
        "            n_neurons = np.prod(shape_out.shape)\n",
        "            n_weights = np.prod(transform.kernel_shape)\n",
        "            print(\n",
        "                \"%s: %s: conv %s, stride %s, output %s (%d neurons, %d weights)\"\n",
        "                % (\n",
        "                    loc,\n",
        "                    name,\n",
        "                    kernel_size,\n",
        "                    strides,\n",
        "                    shape_out.shape,\n",
        "                    n_neurons,\n",
        "                    n_weights,\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            # dense layer\n",
        "            n_neurons = layer_conf.pop(\"n_neurons\")\n",
        "\n",
        "            shape_out = nengo.transforms.ChannelShape((n_neurons,))\n",
        "            transform = nengo.Dense(\n",
        "                (shape_out.size, shape_in.size),\n",
        "                init=nengo_dl.dists.Glorot(),\n",
        "            )\n",
        "\n",
        "            loc = \"chip\" if on_chip else \"host\"\n",
        "            n_weights = np.prod(transform.shape)\n",
        "            print(\n",
        "                \"%s: %s: dense %d, output %s (%d neurons, %d weights)\"\n",
        "                % (loc, name, n_neurons, shape_out.shape, n_neurons, n_weights)\n",
        "            )\n",
        "\n",
        "        assert len(layer_conf) == 0, \"Unused fields in %s: %s\" % (\n",
        "            [name] + list(layer_conf)\n",
        "        )\n",
        "\n",
        "        total_n_neurons += n_neurons\n",
        "        total_n_weights += n_weights\n",
        "\n",
        "        # --- create layer output (Ensemble or Node)\n",
        "        assert on_chip or block is None, \"`block` must be None if off-chip\"\n",
        "\n",
        "        if neuron_type is None:\n",
        "            assert not on_chip, \"Nodes can only be run off-chip\"\n",
        "            y = nengo.Node(size_in=shape_out.size, label=name)\n",
        "        else:\n",
        "            ens = nengo.Ensemble(shape_out.size, 1, neuron_type=neuron_type, label=name)\n",
        "            net.config[ens].on_chip = on_chip\n",
        "            y = ens.neurons\n",
        "\n",
        "            if block is not None:\n",
        "                net.config[ens].block_shape = nengo_loihi.BlockShape(\n",
        "                    block,\n",
        "                    shape_out.shape,\n",
        "                )\n",
        "\n",
        "            # add a probe so we can measure individual layer rates\n",
        "            probe = nengo.Probe(y, synapse=None, label=\"%s_p\" % name)\n",
        "            net.config[probe].keep_history = False\n",
        "            layer_probes.append(probe)\n",
        "\n",
        "        conn = nengo.Connection(x, y, transform=transform)\n",
        "        net.config[conn].pop_type = 32\n",
        "\n",
        "        transforms.append(transform)\n",
        "        connections.append(conn)\n",
        "        x = y\n",
        "        shape_in = shape_out\n",
        "\n",
        "    output_p = nengo.Probe(x, synapse=None, label=\"output_p\")\n",
        "\n",
        "print(\"TOTAL: %d neurons, %d weights\" % (total_n_neurons, total_n_weights))\n",
        "assert len(layer_confs) == len(transforms) == len(connections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Td1AGwIzF4_",
        "outputId": "122ffcf7-e020-420d-d279-6b566eeccdcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "host: input-layer: conv (1, 1), stride (1, 1), output (32, 32, 4) (4096 neurons, 12 weights)\n",
            "chip: conv-layer1: conv (3, 3), stride (2, 2), output (15, 15, 64) (14400 neurons, 2304 weights)\n",
            "chip: conv-layer2: conv (3, 3), stride (1, 1), output (13, 13, 72) (12168 neurons, 41472 weights)\n",
            "chip: conv-layer3: conv (3, 3), stride (2, 2), output (6, 6, 256) (9216 neurons, 165888 weights)\n",
            "chip: conv-layer4: conv (1, 1), stride (1, 1), output (6, 6, 256) (9216 neurons, 65536 weights)\n",
            "chip: conv-layer5: conv (1, 1), stride (1, 1), output (6, 6, 64) (2304 neurons, 16384 weights)\n",
            "chip: dense-layer: dense 100, output (100,) (100 neurons, 230400 weights)\n",
            "host: output-layer: dense 10, output (10,) (10 neurons, 1000 weights)\n",
            "TOTAL: 51510 neurons, 522996 weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train NengoDL\n",
        "Define input and target dictionaries to pass to NengoDL"
      ],
      "metadata": {
        "id": "uT7P5tkkBchx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = {inp: train_x_flat}\n",
        "train_targets = {output_p: train_t_flat}\n",
        "\n",
        "test_inputs = {inp: test_x_flat}\n",
        "test_targets = {output_p: test_t_flat}\n",
        "for probe in layer_probes:\n",
        "    train_targets[probe] = np.zeros((train_t_flat.shape[0], 1, 0), dtype=np.float32)\n",
        "    test_targets[probe] = np.zeros((test_t_flat.shape[0], 1, 0), dtype=np.float32)"
      ],
      "metadata": {
        "id": "DhWeZgZSBjyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- evaluate layers\n",
        "#minibatch_size = 100\n",
        "with nengo_dl.Simulator(net, minibatch_size = 100, progress_bar=False) as sim:\n",
        "    for conf, conn in zip(layer_confs, connections):\n",
        "        weights = sim.model.sig[conn][\"weights\"].initial_value\n",
        "        print(\"%s: initial weights: %0.3f\" % (conf[\"name\"], np.abs(weights).mean()))\n",
        "\n",
        "    sim.run_steps(1, data={inp: train_x_flat[:100]})\n",
        "\n",
        "for conf, layer_probe in zip(layer_confs, layer_probes):\n",
        "    out = sim.data[layer_probe][-1]\n",
        "    print(\"%s: initial rates: %0.3f\" % (conf[\"name\"], np.mean(out)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI6uJ9peN_R_",
        "outputId": "7fc17c4a-dabf-4d6c-a7a8-113e223ed62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nengo_dl/simulator.py:461: UserWarning: No GPU support detected. See https://www.nengo.ai/nengo-dl/installation.html#installing-tensorflow for instructions on setting up TensorFlow with GPU support.\n",
            "  \"No GPU support detected. See \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input-layer: initial weights: 0.581\n",
            "conv-layer1: initial weights: 0.050\n",
            "conv-layer2: initial weights: 0.035\n",
            "conv-layer3: initial weights: 0.023\n",
            "conv-layer4: initial weights: 0.054\n",
            "conv-layer5: initial weights: 0.068\n",
            "dense-layer: initial weights: 0.025\n",
            "output-layer: initial weights: 0.118\n",
            "input-layer: initial rates: 0.476\n",
            "conv-layer1: initial rates: 0.109\n",
            "conv-layer2: initial rates: 0.148\n",
            "conv-layer3: initial rates: 0.110\n",
            "conv-layer4: initial rates: 0.125\n",
            "conv-layer5: initial rates: 0.114\n",
            "dense-layer: initial rates: 0.160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do_training = False\n",
        "\n",
        "checkpoint_base = \"./cifar10_convnet_params\"\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_idg = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    data_format=\"channels_last\" if channels_last else \"channels_first\",\n",
        ")\n",
        "train_idg.fit(train_x)\n",
        "\n",
        "with nengo_dl.Simulator(net, minibatch_size=batch_size) as sim:\n",
        "    percentile = 99.9\n",
        "\n",
        "    def rate_metric(_, outputs):\n",
        "        # take percentile over all examples, for each neuron\n",
        "        top_rates = tfp.stats.percentile(outputs, percentile, axis=(0, 1))\n",
        "        return tf.reduce_mean(top_rates) / amp\n",
        "\n",
        "    losses = collections.OrderedDict()\n",
        "    metrics = collections.OrderedDict()\n",
        "    loss_weights = collections.OrderedDict()\n",
        "\n",
        "    losses[output_p] = tf.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    metrics[output_p] = \"accuracy\"\n",
        "    loss_weights[output_p] = 1.0\n",
        "\n",
        "    for probe, layer_conf in zip(layer_probes, layer_confs):\n",
        "        metrics[probe] = rate_metric\n",
        "\n",
        "        if layer_conf.get(\"on_chip\", True):\n",
        "            losses[probe] = partial(\n",
        "                percentile_l2_loss_range,\n",
        "                min_rate=0.5 * rate_target,\n",
        "                max_rate=rate_target,\n",
        "                percentile=percentile,\n",
        "            )\n",
        "            loss_weights[probe] = rate_reg\n",
        "        else:\n",
        "            losses[probe] = partial(\n",
        "                percentile_l2_loss_range,\n",
        "                min_rate=0,\n",
        "                max_rate=rate_target,\n",
        "                percentile=percentile,\n",
        "            )\n",
        "            loss_weights[probe] = 10 * rate_reg\n",
        "\n",
        "    sim.compile(\n",
        "        loss=losses,\n",
        "        optimizer=tf.optimizers.Adam(),\n",
        "        metrics=metrics,\n",
        "        loss_weights=loss_weights,\n",
        "    )\n",
        "\n",
        "    if do_training:\n",
        "        # --- train\n",
        "        steps_per_epoch = len(train_x) // batch_size\n",
        "\n",
        "        # Create a NengoImageIterator that will return the appropriate dictionaries\n",
        "        # with augmented images. Since we are using a generator, we need to include\n",
        "        # the `n_steps` parameter so that NengoDL knows how many timesteps are in\n",
        "        # each example (in our case, since we just have static images, it's one).\n",
        "        n = steps_per_epoch * batch_size\n",
        "        n_steps = np.ones((n, 1), dtype=np.int32)\n",
        "        train_data = NengoImageIterator(\n",
        "            image_data_generator=train_idg,\n",
        "            x_keys=[inp.label, \"n_steps\"],\n",
        "            x=[train_x[:n], n_steps],\n",
        "            y_keys=[output_p.label] + [probe.label for probe in layer_probes],\n",
        "            y=[train_t[:n]]\n",
        "            + [np.zeros((n, 1, 0), dtype=np.float32) for _ in layer_probes],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "        n_epochs = 100\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            sim.fit(\n",
        "                train_data,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "                epochs=1,\n",
        "                verbose=2,\n",
        "            )\n",
        "\n",
        "            # report test data statistics\n",
        "            outputs = sim.evaluate(x=test_inputs, y=test_targets, verbose=0)\n",
        "            print(\"Epoch %d test: %s\" % (epoch, outputs))\n",
        "\n",
        "            # save the parameters to the checkpoint\n",
        "            savefile = checkpoint_base\n",
        "            sim.save_params(savefile)\n",
        "            print(\"Saved params to %r\" % savefile)\n",
        "    else:\n",
        "        urlretrieve(\n",
        "            \"https://drive.google.com/uc?export=download&\"\n",
        "            \"id=1jvP8IsdqGH2kn0OJOykJxjBsJLgk8GsY\",\n",
        "            \"%s.npz\" % checkpoint_base,\n",
        "        )\n",
        "        sim.load_params(checkpoint_base)\n",
        "        print(\"Loaded params %r\" % checkpoint_base)\n",
        "\n",
        "    # copy the learned/loaded parameters back to the network, for Loihi simulator\n",
        "    sim.freeze_params(net)\n",
        "\n",
        "    # run the network on some of the train and test data to benchmark performance\n",
        "    try:\n",
        "        train_slice = slice(0, 1000)\n",
        "        train_outputs = sim.evaluate(\n",
        "            x=slice_data_dict(train_inputs, train_slice),\n",
        "            y=slice_data_dict(train_targets, train_slice),\n",
        "            verbose=0,\n",
        "        )\n",
        "        print(\"Final train:\")\n",
        "        for key, val in train_outputs.items():\n",
        "            print(\"  %s: %s\" % (key, val))\n",
        "\n",
        "        # test_slice = slice(None)\n",
        "        test_slice = slice(0, 1000)\n",
        "        test_outputs = sim.evaluate(\n",
        "            x=slice_data_dict(test_inputs, test_slice),\n",
        "            y=slice_data_dict(test_targets, test_slice),\n",
        "            verbose=0,\n",
        "        )\n",
        "        print(\"Final test:\")\n",
        "        for key, val in test_outputs.items():\n",
        "            print(\"  %s: %s\" % (key, val))\n",
        "    except Exception as e:\n",
        "        print(\"Could not compute ANN values on this machine: %s\" % e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmO_MvvRPwa4",
        "outputId": "53f6ba67-49cd-4976-b59b-f29d224c7ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build finished in 0:00:00                                                      \n",
            "Optimization finished in 0:00:00                                               \n",
            "Construction finished in 0:00:01                                               \n",
            "Loaded params './cifar10_convnet_params'\n",
            "|             Constructing graph: build stage (0%)             | ETA:  --:--:--"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nengo_dl/simulator.py:1922: UserWarning: Number of elements in input data (1000) is not evenly divisible by Simulator.minibatch_size (256); input data will be truncated.\n",
            "  f\"Number of elements in input data ({data_batch}) is not \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final train:\n",
            "  loss: 0.5278145670890808\n",
            "  input-layer_p_loss: 0.17015397548675537\n",
            "  conv-layer1_p_loss: 3.6382648944854736\n",
            "  conv-layer2_p_loss: 18.71417236328125\n",
            "  conv-layer3_p_loss: 9.682263374328613\n",
            "  conv-layer4_p_loss: 3.000544309616089\n",
            "  conv-layer5_p_loss: 0.7379398345947266\n",
            "  dense-layer_p_loss: 22.423864364624023\n",
            "  output_p_loss: 0.46791598200798035\n",
            "  input-layer_p_rate_metric: 107.9923095703125\n",
            "  conv-layer1_p_rate_metric: 91.8680191040039\n",
            "  conv-layer2_p_rate_metric: 119.3670425415039\n",
            "  conv-layer3_p_rate_metric: 127.5032958984375\n",
            "  conv-layer4_p_rate_metric: 114.98700714111328\n",
            "  conv-layer5_p_rate_metric: 114.4601058959961\n",
            "  dense-layer_p_rate_metric: 246.77777099609375\n",
            "  output_p_accuracy: 0.8463541865348816\n",
            "Final test:\n",
            "  loss: 0.6979226469993591\n",
            "  input-layer_p_loss: 0.0004697335243690759\n",
            "  conv-layer1_p_loss: 3.68471622467041\n",
            "  conv-layer2_p_loss: 17.730581283569336\n",
            "  conv-layer3_p_loss: 9.436734199523926\n",
            "  conv-layer4_p_loss: 3.0312061309814453\n",
            "  conv-layer5_p_loss: 0.6052677631378174\n",
            "  dense-layer_p_loss: 22.224279403686523\n",
            "  output_p_loss: 0.6412052512168884\n",
            "  input-layer_p_rate_metric: 104.85063934326172\n",
            "  conv-layer1_p_rate_metric: 89.22180938720703\n",
            "  conv-layer2_p_rate_metric: 119.38201904296875\n",
            "  conv-layer3_p_rate_metric: 127.10941314697266\n",
            "  conv-layer4_p_rate_metric: 114.9713363647461\n",
            "  conv-layer5_p_rate_metric: 114.35720825195312\n",
            "  dense-layer_p_rate_metric: 246.05555725097656\n",
            "  output_p_accuracy: 0.78125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove layer probes\n",
        "for probe in layer_probes:\n",
        "    if probe in net.probes:\n",
        "        net.probes.remove(probe)\n",
        "\n",
        "# add synapses to connections\n",
        "for conn in net.all_connections:\n",
        "    conn.synapse = nengo.synapses.Lowpass(0.01)\n",
        "\n",
        "n_images = 10\n",
        "\n",
        "sim_time = n_images * presentation_time\n",
        "\n",
        "with nengo_loihi.Simulator(net) as sim:\n",
        "    # print information about how cores are being utilized\n",
        "    print(\"\\n\".join(sim.model.utilization_summary()))\n",
        "\n",
        "    sim.run(sim_time)"
      ],
      "metadata": {
        "id": "Jl2Vb98NhHKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f13ceb6-29bf-45d5-8478-81d30b696d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoihiBlock(<Ensemble 'conv-layer1'>[0]): 100.0% compartments, 7.1% in-axons, 35.6% out-axons, 8.1% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[1]): 100.0% compartments, 7.1% in-axons, 35.6% out-axons, 8.1% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[2]): 100.0% compartments, 7.1% in-axons, 35.6% out-axons, 8.1% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[3]): 100.0% compartments, 7.1% in-axons, 35.6% out-axons, 8.1% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[4]): 87.5% compartments, 6.2% in-axons, 31.6% out-axons, 7.4% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[5]): 87.5% compartments, 6.2% in-axons, 31.6% out-axons, 7.4% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[6]): 87.5% compartments, 6.2% in-axons, 31.6% out-axons, 7.4% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[7]): 87.5% compartments, 6.2% in-axons, 31.6% out-axons, 7.4% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[8]): 87.5% compartments, 6.2% in-axons, 31.6% out-axons, 7.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[9]): 87.5% compartments, 6.2% in-axons, 31.6% out-axons, 7.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[10]): 87.5% compartments, 6.2% in-axons, 31.6% out-axons, 7.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[11]): 87.5% compartments, 6.2% in-axons, 31.6% out-axons, 7.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[12]): 76.6% compartments, 5.5% in-axons, 28.1% out-axons, 6.7% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[13]): 76.6% compartments, 5.5% in-axons, 28.1% out-axons, 6.7% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[14]): 76.6% compartments, 5.5% in-axons, 28.1% out-axons, 6.7% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer1'>[15]): 76.6% compartments, 5.5% in-axons, 28.1% out-axons, 6.7% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[0]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[1]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[2]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[3]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[4]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[5]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[6]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[7]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[8]): 38.3% compartments, 2.0% in-axons, 52.6% out-axons, 87.0% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[9]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[10]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[11]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[12]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[13]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[14]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[15]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[16]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[17]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[18]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[19]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[20]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[21]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[22]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[23]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[24]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[25]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[26]): 32.8% compartments, 1.8% in-axons, 45.1% out-axons, 76.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[27]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[28]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[29]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[30]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[31]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[32]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[33]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[34]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer2'>[35]): 28.1% compartments, 1.6% in-axons, 38.7% out-axons, 73.8% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[0]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[1]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[2]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[3]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[4]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[5]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[6]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[7]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[8]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[9]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[10]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[11]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[12]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[13]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[14]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[15]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[16]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[17]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[18]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[19]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[20]): 42.2% compartments, 4.1% in-axons, 19.3% out-axons, 59.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer3'>[21]): 14.1% compartments, 4.1% in-axons, 19.3% out-axons, 10.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[0]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[1]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[2]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[3]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[4]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[5]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[6]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[7]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[8]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[9]): 84.4% compartments, 0.9% in-axons, 5.3% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer4'>[10]): 56.2% compartments, 0.9% in-axons, 5.3% out-axons, 22.3% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer5'>[0]): 84.4% compartments, 0.9% in-axons, 42.2% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer5'>[1]): 84.4% compartments, 0.9% in-axons, 42.2% out-axons, 44.5% synapses\n",
            "LoihiBlock(<Ensemble 'conv-layer5'>[2]): 56.2% compartments, 0.9% in-axons, 28.1% out-axons, 22.3% synapses\n",
            "LoihiBlock(<Ensemble 'dense-layer'>[0:50:1]): 4.9% compartments, 56.2% in-axons, 0.0% out-axons, 98.4% synapses\n",
            "LoihiBlock(<Ensemble 'dense-layer'>[50:100:1]): 4.9% compartments, 56.2% in-axons, 0.0% out-axons, 98.4% synapses\n",
            "Average (90 blocks): 51.4% compartments, 4.2% in-axons, 30.4% out-axons, 55.3% synapses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pres_steps = int(presentation_time / sim.dt)\n",
        "class_steps = int(0.3 * pres_steps)\n",
        "\n",
        "output = sim.data[output_p]\n",
        "output = output.reshape((n_images, pres_steps) + output[0].shape)\n",
        "output = output[:, -class_steps:].mean(axis=1)\n",
        "preds = np.argmax(output, axis=-1)\n",
        "\n",
        "assert preds.shape == test_y[:n_images].shape\n",
        "\n",
        "print(\"Predictions: %s\" % (list(preds),))\n",
        "print(\"Actual:      %s\" % (list(test_y[:n_images]),))\n",
        "error = (preds != test_y[:n_images]).mean()\n",
        "print(\"Accuracy: %0.3f%%, Error: %0.3f%%\" % (100 * (1 - error), 100 * error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZd8UkHwhinu",
        "outputId": "feba1f2b-3d3f-4d90-cbdc-c38a0bfd1032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [3, 8, 8, 0, 6, 6, 1, 6, 3, 9]\n",
            "Actual:      [3, 8, 8, 0, 6, 6, 1, 6, 3, 1]\n",
            "Accuracy: 90.000%, Error: 10.000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "images = test_x if channels_last else np.transpose(test_x, (0, 2, 3, 1))\n",
        "ni, nj, nc = images[0].shape\n",
        "allimage = np.zeros((ni, nj * n_images, nc))\n",
        "for i, image in enumerate(images[:n_images]):\n",
        "    allimage[:, i * nj : (i + 1) * nj] = image\n",
        "if allimage.shape[-1] == 1:\n",
        "    allimage = allimage[:, :, 0]\n",
        "allimage = (allimage + 1) / 2  # scale to [0, 1]\n",
        "plt.imshow(allimage, aspect=\"auto\", interpolation=\"none\", cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "t = sim.trange()\n",
        "plt.plot(t, sim.data[output_p])\n",
        "plt.xlim([t[0], t[-1]])\n",
        "plt.xlabel(\"time [s]\")\n",
        "plt.legend(label_names, loc=\"upper right\", bbox_to_anchor=(1.18, 1.05))"
      ],
      "metadata": {
        "id": "aFAvLgNMjM91",
        "outputId": "72d6d458-3ce3-4ae3-b807-8350f659aaf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-77c889b75640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchannels_last\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    }
  ]
}